volumes:
  shared-workspace:
    name: "hadoop-distributed-file-system"
    driver: local
  spark_job:
    driver: local
services:
  spark-master:
    image: bitnami/spark:3.5.4
    container_name: spark-master
    ports:
      - 4040:8080
      - 7077:7077
    volumes:
      - ./spark_job:/opt/bitnami/spark/jobs # 로컬 디렉토리를 정확히 매핑
    environment:
      - SPARK_MODE=master # 명시적으로 Master 모드 설정
  spark-worker-1:
    image: bitnami/spark:3.5.4
    container_name: spark-worker-1
    environment:
      - SPARK_MODE=worker # 명시적으로 Worker 모드 설정
      - SPARK_MASTER=spark://spark-master:7077 # Master URL 설정
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    ports:
      - 4041:8081
    volumes:
      - ./spark_job:/opt/bitnami/spark/jobs # 로컬 디렉토리를 정확히 매핑
    depends_on:
      - spark-master
  spark-worker-2:
    image: bitnami/spark:3.5.4
    container_name: spark-worker-2
    environment:
      - SPARK_MODE=worker # 명시적으로 Worker 모드 설정
      - SPARK_MASTER=spark://spark-master:7077 # Master URL 설정
      - SPARK_WORKER_CORES=1
      - SPARK_WORKER_MEMORY=512m
    volumes:
      - ./spark_job:/opt/bitnami/spark/jobs # 로컬 디렉토리를 정확히 매핑
    ports:
      - 4042:8081
    depends_on:
      - spark-master
