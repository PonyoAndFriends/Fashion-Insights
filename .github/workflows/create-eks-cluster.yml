name: Deploy EKS Cluster with Airflow and Spark

on:
  workflow_dispatch:  # 수동으로 트리거되는 워크플로우

jobs:
  deploy-eks-cluster:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout Code
      uses: actions/checkout@v3

    - name: Set up AWS credentials
      uses: aws-actions/configure-aws-credentials@v2
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ secrets.AWS_REGION }}
        AWS_USER_ARN: ${{ secrets.AWS_USER_ARN }}

    - name: Set up Python environment
      uses: actions/setup-python@v4
      with:
        python-version: '3.8'

    - name: Install AWS CLI and kubectl
      run: |
        curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
        unzip awscliv2.zip
        sudo ./aws/install --update
        curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/


    - name: Set up Node.js and install AWS CDK
      uses: actions/setup-node@v3
      with:
        node-version: '20'  # 필요한 Node.js 버전
    - run: |
        npm install -g aws-cdk  # AWS CDK CLI 설치

    - name: Install AWS CDK
      run: |
        cd aws_cdk  # CDK 코드가 위치한 디렉토리로 이동
        python3 -m pip install -r requirements.txt  # CDK 의존성 설치

    - name: Deploy EKS Cluster with CDK
      run: |
        cd aws_cdk
        cdk bootstrap aws://${{ secrets.AWS_ACCOUNT_ID }}/${{ secrets.AWS_REGION }}  # CDK 부트스트랩 실행
        cdk deploy --app "python3 app.py" --require-approval never --context github_actions_user_arn=${{ secrets.AWS_USER_ARN }}

    - name: Get EKS cluster name
      id: get-cluster-name
      run: |
        CLUSTER_NAME=$(aws eks list-clusters --query "clusters[0]" --output text)
        echo "CLUSTER_NAME=$CLUSTER_NAME" >> $GITHUB_ENV

    - name: Set up kubeconfig
      run: |
        aws eks update-kubeconfig --name ${{ env.CLUSTER_NAME }} --region ${{ secrets.AWS_REGION }}

    - name: Install Helm
      run: |
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
    
    - name: Create Spark Service Account and RBAC
      run: |
        kubectl apply -f ./spark/kubernetes/spark-service-account.yaml
        kubectl apply -f ./spark/kubernetes/spark-role.yaml

    - name: Install Airflow via Helm
      run: |
        helm repo add apache-airflow https://airflow.apache.org
        helm repo update
        helm install airflow apache-airflow/airflow \
          --namespace airflow \
          --create-namespace \
          --set dags.persistence.enabled=true \
          --set airflow.executor=CeleryExecutor \
          --set airflow.image.repository=apache/airflow
          --set airflow.image.tag=2.9.1 \
          --set airflow.image.pullPolicy=IfNotPresent \
          --set workers.replicas=2
          --set serviceAccount.name=my-eks-service-account
          --set airflow.config.AIRFLOW__CORE__LOAD_EXAMPLES=false

    - name: Install Spark via Helm
      run: |
        helm repo add bitnami https://charts.bitnami.com/bitnami
        helm repo update
        helm install spark bitnami/spark \
          --namespace spark \
          --create-namespace \
          --set serviceAccount.name=my-eks-service-account \
          --set spark.master=k8s://https://kubernetes.default.svc \
          --set spark.worker.instances=3

    - name: Copy DAGs to Airflow
      run: |
        kubectl create configmap airflow-dags --from-file=./airflow/airflow_dags -n airflow
        kubectl patch deployment airflow-scheduler -n airflow \
          --type='json' -p='[{"op": "add", "path": "/spec/template/spec/volumes/-", "value": {"name": "airflow-dags", "configMap": {"name": "airflow-dags"}}}, {"op": "add", "path": "/spec/template/spec/containers/0/volumeMounts/-", "value": {"name": "airflow-dags", "mountPath": "/opt/airflow/dags"}}]'

    - name: Mount Spark Jobs
      run: |
        kubectl create configmap spark-jobs --from-file=./spark/spark_jobs -n spark
        kubectl patch deployment spark-master -n spark \
          --type='json' -p='[{"op": "add", "path": "/spec/template/spec/volumes/-", "value": {"name": "spark-jobs", "configMap": {"name": "spark-jobs"}}}, {"op": "add", "path": "/spec/template/spec/containers/0/volumeMounts/-", "value": {"name": "spark-jobs", "mountPath": "/mnt/jobs"}}]'

    - name: Add Kubernetes connection to Airflow
      run: |
        airflow connections add 'kubernetes_default' \
          --conn-type 'Kubernetes' \
          --conn-host 'https://kubernetes.default.svc' \
          --conn-extra '{"kube_config": "/root/.kube/config"}'

    - name: Verify Deployment
      run: |
        kubectl get pods --namespace airflow
        kubectl get pods --namespace spark
        echo "Deployment Complete!"
